{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize Images from google images to 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# downloadsdir = './downloads'\n",
    "# dirs = [x for x in os.walk(downloadsdir)]\n",
    "# for shroom in dirs[0][1]:\n",
    "#     resizeddir = './resized\\\\\\\\'+shroom\n",
    "#     fullsizedir = './downloads\\\\\\\\'+shroom\n",
    "#     if not os.path.exists(resizeddir):\n",
    "#         os.makedirs(resizeddir)\n",
    "#     pics = [x[2] for x in os.walk(fullsizedir)]\n",
    "#     for idx, p in enumerate(pics[0]):\n",
    "#         if not p.endswith('.gif'):\n",
    "#             picpath = fullsizedir+'\\\\\\\\'+p\n",
    "#             try:\n",
    "#                 img = cv2.imread(picpath,-1)\n",
    "#                 resized = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "#                 cv2.imwrite(resizeddir+'\\\\\\\\'+str(idx)+'.jpg', resized)\n",
    "#             except:\n",
    "#                 print('Could not process ' + p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010, 224, 224, 3)\n",
      "(1010,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "num_classes = 16\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "# is_edible = []\n",
    "\n",
    "# mushroom_info = pd.read_json('mushroom_classes.json', lines=True)\n",
    "classdirs = [x for x in os.walk('./resized')]\n",
    "for shroomidx, shroom in enumerate(classdirs[0][1]):\n",
    "#     info = mushroom_info.loc[mushroom_info.name_latin == shroom]\n",
    "#     edible = info.edibility.isin((\"edible\", \"edible and good\", \"edible and excellent\"))\n",
    "#     is_edible.append(edible)\n",
    "    imagepaths = [x for x in os.walk('./resized\\\\\\\\'+shroom)]\n",
    "    for path in imagepaths[0][2]:\n",
    "        img = cv2.imread('./resized\\\\\\\\'+shroom+'\\\\\\\\'+path,-1)\n",
    "        if (img.shape == (224, 224, 3)):\n",
    "            X.append(img)\n",
    "            y.append(shroomidx)\n",
    "\n",
    "X = np.stack(X)\n",
    "y = np.stack(y)\n",
    "# is_edible = pd.Series(is_edible)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# print(is_edible.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data\n",
    "import keras\n",
    "# rescale !!\n",
    "X = X/255.0\n",
    "\n",
    "N = len(X)\n",
    "N_tr = int(0.8*N)\n",
    "\n",
    "# shuffle the data\n",
    "indx = np.arange(N)\n",
    "np.random.shuffle(indx)\n",
    "X = X[indx]\n",
    "y = y[indx]\n",
    "\n",
    "# split\n",
    "X_tr = X[0:N_tr]\n",
    "y_tr = y[0:N_tr]\n",
    "y_tr = keras.utils.to_categorical(y_tr, num_classes)\n",
    "\n",
    "X_te = X[N_tr:]\n",
    "y_te = y[N_tr:]\n",
    "y_te = keras.utils.to_categorical(y_te, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "def my_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=X[0].shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model()\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# basemodel = VGG16(include_top=False,\n",
    "#                   input_shape=(224,224,3))\n",
    "# # for layer in basemodel.layers[1:13]:\n",
    "# #     layer.trainable = False\n",
    "    \n",
    "# topmodel = Sequential()\n",
    "# topmodel.add(Flatten(input_shape=basemodel.output_shape[1:]))\n",
    "# topmodel.add(Dense(num_classes, activation='relu'))\n",
    "# topmodel.add(Dropout(0.4))\n",
    "# topmodel.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "# model = Model(inputs = basemodel.input, outputs = topmodel(basemodel.output))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 220, 220, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 108, 108, 128)     36992     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 106, 106, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 51, 51, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 49, 49, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                663584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 3,514,320\n",
      "Trainable params: 3,514,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 808 samples, validate on 202 samples\n",
      "Epoch 1/200\n",
      "808/808 [==============================] - 16s 20ms/step - loss: 2.7723 - acc: 0.0656 - val_loss: 2.7728 - val_acc: 0.0396\n",
      "Epoch 2/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7710 - acc: 0.0903 - val_loss: 2.7735 - val_acc: 0.0446\n",
      "Epoch 3/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7707 - acc: 0.0780 - val_loss: 2.7740 - val_acc: 0.0446\n",
      "Epoch 4/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7705 - acc: 0.0705 - val_loss: 2.7740 - val_acc: 0.0446\n",
      "Epoch 5/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7690 - acc: 0.0817 - val_loss: 2.7746 - val_acc: 0.0446\n",
      "Epoch 6/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7695 - acc: 0.0569 - val_loss: 2.7746 - val_acc: 0.0446\n",
      "Epoch 7/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7667 - acc: 0.0755 - val_loss: 2.7749 - val_acc: 0.0446\n",
      "Epoch 8/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7665 - acc: 0.0829 - val_loss: 2.7754 - val_acc: 0.0446\n",
      "Epoch 9/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7655 - acc: 0.0804 - val_loss: 2.7759 - val_acc: 0.0446\n",
      "Epoch 10/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7642 - acc: 0.0817 - val_loss: 2.7773 - val_acc: 0.0446\n",
      "Epoch 11/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7638 - acc: 0.0780 - val_loss: 2.7771 - val_acc: 0.0446\n",
      "Epoch 12/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7642 - acc: 0.0817 - val_loss: 2.7776 - val_acc: 0.0446\n",
      "Epoch 13/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7600 - acc: 0.0891 - val_loss: 2.7788 - val_acc: 0.0446\n",
      "Epoch 14/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7630 - acc: 0.0718 - val_loss: 2.7784 - val_acc: 0.0446\n",
      "Epoch 15/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7600 - acc: 0.0842 - val_loss: 2.7786 - val_acc: 0.0446\n",
      "Epoch 16/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7588 - acc: 0.0718 - val_loss: 2.7798 - val_acc: 0.0446\n",
      "Epoch 17/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7635 - acc: 0.0668 - val_loss: 2.7774 - val_acc: 0.0446\n",
      "Epoch 18/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7639 - acc: 0.0804 - val_loss: 2.7772 - val_acc: 0.0446\n",
      "Epoch 19/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7585 - acc: 0.0854 - val_loss: 2.7782 - val_acc: 0.0446\n",
      "Epoch 20/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7637 - acc: 0.0916 - val_loss: 2.7763 - val_acc: 0.0446\n",
      "Epoch 21/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7607 - acc: 0.0916 - val_loss: 2.7752 - val_acc: 0.0446\n",
      "Epoch 22/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7579 - acc: 0.0854 - val_loss: 2.7763 - val_acc: 0.0446\n",
      "Epoch 23/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7585 - acc: 0.0854 - val_loss: 2.7767 - val_acc: 0.0446\n",
      "Epoch 24/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7571 - acc: 0.0743 - val_loss: 2.7747 - val_acc: 0.0347\n",
      "Epoch 25/200\n",
      "808/808 [==============================] - 12s 15ms/step - loss: 2.7601 - acc: 0.0804 - val_loss: 2.7721 - val_acc: 0.0495\n",
      "Epoch 26/200\n",
      "800/808 [============================>.] - ETA: 0s - loss: 2.7508 - acc: 0.0950"
     ]
    }
   ],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "N_BATCH = 16\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.85,\n",
    "                              patience=5,\n",
    "                              min_lr=0.001)\n",
    "history = model.fit(X_tr, y_tr,\n",
    "                    batch_size = N_BATCH,\n",
    "                    epochs = 200,\n",
    "                    verbose = 1,\n",
    "                    validation_data = (X_te, y_te),\n",
    "                    callbacks=[reduce_lr])\n",
    "\n",
    "# save model weights and training history\n",
    "model.save_weights('models/weights.h5')\n",
    "import pickle\n",
    "with open('models/history', 'wb') as f:\n",
    "        pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model training history\n",
    "import matplotlib.pyplot as plt\n",
    "with open(\"models/history\", \"rb\") as f:\n",
    "    hist = pickle.load(f)\n",
    "    \n",
    "plt.plot(hist[\"acc\"], label = \"train accuracy\", color = \"green\")\n",
    "plt.plot(hist[\"val_acc\"], label = \"test accuracy\", color = \"blue\")\n",
    "plt.legend()\n",
    "plt.savefig(\"docs/training_history.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/model1.h5\")\n",
    "# def predict_and_draw(i, X, y, model, classnames, save = False):\n",
    "#     x = X[i]\n",
    "#     x = x.astype(np.uint8)\n",
    "#     plt.imshow(x)\n",
    "#     x = x/ 255.0\n",
    "#     x.shape = (1, ) + x.shape\n",
    "#     p = model.predict(x)[0,0].round(2)\n",
    "#     plt.title(\"P(edible): \" + str(p) + \" Actually edible: \" + str(y[i]))\n",
    "#     if(save):\n",
    "#         plt.savefig(\"docs/prediction_\"+str(i)+\".png\")\n",
    "#     plt.show()\n",
    "    \n",
    "# draw_sample(1, X, y, classdirs[0][1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
